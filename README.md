## Wikipedia Graph Analysis Project

This project provides a pipeline for downloading, parsing, and cleaning Wikipedia dumps to extract article text and outgoing links. This data can then be used for various graph analysis tasks.

### ğŸ”§ Setup InstructionsFollow these steps to set up your project environment.

#### 1. Activate Virtual Environment (Windows PowerShell)

It's recommended to use a virtual environment to manage project dependencies..

``` powershell
\venv\Scripts\Activate.ps1
```

##### Install Dependencies

Install the necessary Python libraries using the provided requirements.txt file.

``` powershell
pip install -r requirements.txt
```


### ğŸ—‚ï¸ Project Structure

The project is organized as follows:

<pre lang="markdown"> 
    <code>
    ```text
     project/ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ 00_raw/ # (Optional) Compressed Wikipedia dump(s) â”‚ â”œâ”€â”€ 10_parsed/ # Stores title + raw Wikitext (Parquet format) â”‚ â””â”€â”€ 20_clean/ # Stores clean_body + destination articles (Parquet format) â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ 00_ingest.py # Script to download Wikipedia dumps â”‚ â”œâ”€â”€ 10_parse.py # Script to parse XML to Parquet â”‚ â”œâ”€â”€ 20_transform.py # Script to clean markup and extract links â”‚ â””â”€â”€ utils/ â”‚ â”œâ”€â”€ stream_bz2.py # Utility for streaming bz2 compressed files â”‚ â””â”€â”€ wikiclean.py # Utility for cleaning Wikitext â””â”€â”€ README.md # This file 
     ```
    </code> 
</pre>

project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 00_raw/         # (Optional) Compressed Wikipedia dump(s)
â”‚   â”œâ”€â”€ 10_parsed/      # Stores title + raw Wikitext (Parquet format)
â”‚   â””â”€â”€ 20_clean/       # Stores clean_body + destination articles (Parquet format)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 00_ingest.py    # Script to download Wikipedia dumps
â”‚   â”œâ”€â”€ 10_parse.py     # Script to parse XML to Parquet
â”‚   â”œâ”€â”€ 20_transform.py # Script to clean markup and extract links
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ stream_bz2.py # Utility for streaming bz2 compressed files
â”‚       â””â”€â”€ wikiclean.py  # Utility for cleaning Wikitext
â””â”€â”€ README.md           # This file


ğŸš€ Running the PipelineThe data processing pipeline consists of three main stages. Each stage writes its output to the data/ directory and is idempotent, meaning if the target file already exists, rerunning the script will verify the timestamp and exit without reprocessing.Step 1: Ingest DataDownload the latest Simple English Wikipedia dump (approximately 60 MB). This step only needs to be run once, or you can skip it if you plan to parse from a URL directly in the next step.python src/00_ingest.py
Output: Potentially stores downloaded dump in data/00_raw/Step 2: Parse XML to ParquetParse the downloaded XML dump (or directly from a URL) into a Parquet file containing article titles and raw Wikitext. This stage produces a file of approximately 230 MB with around 140,000 pages.python src/10_parse.py
Output: data/10_parsed/articles.parquetStep 3: Transform and Clean DataClean the Wikitext markup from the parsed data and extract outgoing links from each article.python src/20_transform.py
Output: data/20_clean/articles.parquetExecution SummaryTo run the entire pipeline:# 1. Download the data (run once, or skip if parsing from URL)
python src/00_ingest.py

# 2. Parse XML to Parquet
python src/10_parse.py

# 3. Clean markup and extract outgoing links
python src/20_transform.py
